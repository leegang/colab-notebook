{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leegang/colab-notebook/blob/main/vts_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple video to translate and lipsync pipeline\n"
      ],
      "metadata": {
        "id": "F3Je_0UIMWw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload a video of someone speaking and get it back in another language, same voice and lip synched.\n",
        "\n",
        "Just a fun proof of concept pipeline, will probably reduce bloat and actually implement later.\n",
        "\n",
        "High quality reduces the gaussian blur significantly and I think it looks much better but it takes a lot longer.\n",
        "\n",
        "Credits to:\n",
        "\n",
        "\n",
        "*   https://github.com/coqui-ai/TTS\n",
        "*   https://github.com/OpenTalker/video-retalking/\n",
        "*   https://github.com/openai/whisper\n",
        "*   https://github.com/justinjohn0306/Wav2Lip/\n",
        "\n",
        "Notes:\n",
        "\n",
        "- 60s is the max it can go, afaik.\n",
        "- mileage may vary with voice synthesis, if it doesnt sound quite right just regenerate\n",
        "- if it crashes even after freeing the memory from the other models, just restart, upload the video again and run only the lip sync part\n",
        "- if you find any bugs pls dm me since i did this at 3am @yeswondwerr"
      ],
      "metadata": {
        "id": "lX-yJyoYfN3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: extract, translate and generate voice"
      ],
      "metadata": {
        "id": "0WYUNzWIMet3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dependencies\n",
        "\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!pip install TTS\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install jiwer\n",
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GUcXjYZ8O8Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload Video\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "uploaded = None\n",
        "resize_to_720p = False\n",
        "\n",
        "def upload_video():\n",
        "  global uploaded\n",
        "  global video_path  # Declare video_path as global to modify it\n",
        "  uploaded = files.upload()\n",
        "  for filename in uploaded.keys():\n",
        "    print(f'Uploaded {filename}')\n",
        "    if resize_to_720p:\n",
        "        filename = resize_video(filename)  # Get the name of the resized video\n",
        "    video_path = filename  # Update video_path with either original or resized filename\n",
        "    return filename\n",
        "\n",
        "\n",
        "def resize_video(filename):\n",
        "    output_filename = f\"resized_{filename}\"\n",
        "    cmd = f\"ffmpeg -i {filename} -vf scale=-1:720 {output_filename}\"\n",
        "    subprocess.run(cmd, shell=True)\n",
        "    print(f'Resized video saved as {output_filename}')\n",
        "    return output_filename\n",
        "\n",
        "# Create a form button that calls upload_video when clicked and a checkbox for resizing\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "button = widgets.Button(description=\"Upload Video\")\n",
        "checkbox = widgets.Checkbox(value=False, description='Resize to 720p (better results)')\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_button_clicked(b):\n",
        "  with output:\n",
        "    global video_path\n",
        "    global resize_to_720p\n",
        "    resize_to_720p = checkbox.value\n",
        "    video_path = upload_video()\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "display(checkbox, button, output)\n"
      ],
      "metadata": {
        "id": "RPkTslMrMzoF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Audio extraction (24 bit) and whisper conversion\n",
        "import subprocess\n",
        "\n",
        "# Ensure video_path variable exists and is not None\n",
        "if 'video_path' in globals() and video_path is not None:\n",
        "    ffmpeg_command = f\"ffmpeg -i '{video_path}' -acodec pcm_s24le -ar 48000 -q:a 0 -map a -y 'output_audio.wav'\"\n",
        "    subprocess.run(ffmpeg_command, shell=True)\n",
        "else:\n",
        "    print(\"No video uploaded. Please upload a video first.\")\n",
        "\n",
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(\"output_audio.wav\")\n",
        "\n",
        "whisper_text = result[\"text\"]\n",
        "whisper_language = result['language']\n",
        "\n",
        "print(\"Whisper text:\", whisper_text)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "j24mTHsxNH0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Translation\n",
        "# Mapping between full names and ISO 639-1 codes\n",
        "language_mapping = {\n",
        "    'English': 'en',\n",
        "    'Spanish': 'es',\n",
        "    'French': 'fr',\n",
        "    'German': 'de',\n",
        "    'Italian': 'it',\n",
        "    'Portuguese': 'pt',\n",
        "    'Polish': 'pl',\n",
        "    'Turkish': 'tr',\n",
        "    'Russian': 'ru',\n",
        "    'Dutch': 'nl',\n",
        "    'Czech': 'cs',\n",
        "    'Arabic': 'ar',\n",
        "    'Chinese (Simplified)': 'zh-cn'\n",
        "}\n",
        "\n",
        "# Dropdown with full names\n",
        "target_language = \"English\" #@param [\"English\", \"Spanish\", \"French\", \"German\", \"Italian\", \"Portuguese\", \"Polish\", \"Turkish\", \"Russian\", \"Dutch\", \"Czech\", \"Arabic\", \"Chinese (Simplified)\"]\n",
        "\n",
        "# Convert full name to ISO 639-1 code\n",
        "target_language_code = language_mapping[target_language]\n",
        "\n",
        "# Assume whisper_text and whisper_language are defined from previous code\n",
        "from googletrans import Translator\n",
        "\n",
        "# Initialize the translator\n",
        "translator = Translator()\n",
        "\n",
        "# Translate the text\n",
        "translated_text = translator.translate(whisper_text, src=whisper_language, dest=target_language_code).text\n",
        "\n",
        "# Output the translated text\n",
        "print(\"Translated text:\", translated_text)\n"
      ],
      "metadata": {
        "id": "XiugF6HdNuLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Voice synthesis\n",
        "from TTS.api import TTS\n",
        "import torch\n",
        "from IPython.display import Audio, display  # Import the Audio and display modules\n",
        "\n",
        "# Initialize TTS\n",
        "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v1\", gpu=True)\n",
        "\n",
        "# Generate audio file\n",
        "tts.tts_to_file(translated_text,\n",
        "    speaker_wav='output_audio.wav',\n",
        "    file_path=\"output_synth.wav\",\n",
        "    language=target_language_code\n",
        ")\n",
        "\n",
        "# Display audio widget to play the generated audio\n",
        "audio_widget = Audio(filename=\"output_synth.wav\", autoplay=False)\n",
        "display(audio_widget)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "t68u8T_IpA4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Delete tts and whisper models before lip sync if you are on T4\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    del tts\n",
        "except NameError:\n",
        "    print(\"Voice model already deleted\")\n",
        "\n",
        "try:\n",
        "    del model\n",
        "except NameError:\n",
        "    print(\"Whisper model already deleted\")\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kYPqmo0gYQIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: generate lip synched video"
      ],
      "metadata": {
        "id": "F13FTIGoVGmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### High Quality (very slow, aprox 15 min to install dependencies and 12 more for vid on T4)"
      ],
      "metadata": {
        "id": "LaN9kFg9Oa-r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBs6cdPiFFUT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Dependencies\n",
        "%cd /content/\n",
        "\n",
        "!git clone https://github.com/vinthony/video-retalking.git &> /dev/null\n",
        "\n",
        "!sudo apt-get install -y libblas-dev liblapack-dev libx11-dev libopenblas-dev\n",
        "\n",
        "!git clone https://github.com/davisking/dlib.git\n",
        "\n",
        "!pip install basicsr==1.4.2 face-alignment==1.3.4 kornia==0.5.1 ninja==1.10.2.3 einops==0.4.1 facexlib==0.2.5 librosa==0.9.2 build\n",
        "\n",
        "!cd dlib && python setup.py install\n",
        "\n",
        "%cd /content/video-retalking\n",
        "\n",
        "!mkdir ./checkpoints\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/30_net_gen.pth -O ./checkpoints/30_net_gen.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/BFM.zip -O ./checkpoints/BFM.zip\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/DNet.pt -O ./checkpoints/DNet.pt\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/ENet.pth -O ./checkpoints/ENet.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/expression.mat -O ./checkpoints/expression.mat\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/face3d_pretrain_epoch_20.pth -O ./checkpoints/face3d_pretrain_epoch_20.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/GFPGANv1.3.pth -O ./checkpoints/GFPGANv1.3.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/GPEN-BFR-512.pth -O ./checkpoints/GPEN-BFR-512.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/LNet.pth -O ./checkpoints/LNet.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/ParseNet-latest.pth -O ./checkpoints/ParseNet-latest.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/RetinaFace-R50.pth -O ./checkpoints/RetinaFace-R50.pth\n",
        "!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/shape_predictor_68_face_landmarks.dat -O ./checkpoints/shape_predictor_68_face_landmarks.dat\n",
        "!unzip -d ./checkpoints/BFM ./checkpoints/BFM.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate video\n",
        "\n",
        "%cd /content/video-retalking\n",
        "\n",
        "video_path_fix = f\"'../{video_path}'\"\n",
        "\n",
        "!python inference.py \\\n",
        "  --face $video_path_fix \\\n",
        "  --audio \"/content/output_synth.wav\" \\\n",
        "  --outfile '/content/output_high_qual.mp4'"
      ],
      "metadata": {
        "id": "SXoxhSJ-L_3i",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normal quality (around 5 min on T4)"
      ],
      "metadata": {
        "id": "kxQrW9mJPBg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dependencies\n",
        "%cd /content/\n",
        "\n",
        "!git clone https://github.com/justinjohn0306/Wav2Lip\n",
        "!cd Wav2Lip && pip install -r requirements_colab.txt\n",
        "\n",
        "%cd /content/Wav2Lip\n",
        "\n",
        "!wget \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" -O \"face_detection/detection/sfd/s3fd.pth\"\n",
        "!wget 'https://github.com/justinjohn0306/Wav2Lip/releases/download/models/wav2lip.pth' -O 'checkpoints/wav2lip.pth'\n",
        "!wget 'https://github.com/justinjohn0306/Wav2Lip/releases/download/models/wav2lip_gan.pth' -O 'checkpoints/wav2lip_gan.pth'\n",
        "!wget 'https://github.com/justinjohn0306/Wav2Lip/releases/download/models/resnet50.pth' -O 'checkpoints/resnet50.pth'\n",
        "!wget 'https://github.com/justinjohn0306/Wav2Lip/releases/download/models/mobilenet.pth' -O 'checkpoints/mobilenet.pth'\n",
        "\n",
        "!pip install batch-face"
      ],
      "metadata": {
        "id": "uIeFRqQYPEwS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate video\n",
        "\n",
        "%cd /content/Wav2Lip\n",
        "\n",
        "#This is the detection box padding, if you see it doesnt sit quite right, just adjust the values a bit. Usually the bottom one is the biggest issue\n",
        "pad_top =  0\n",
        "pad_bottom =  15\n",
        "pad_left =  0\n",
        "pad_right =  0\n",
        "rescaleFactor =  1\n",
        "\n",
        "video_path_fix = f\"'../{video_path}'\"\n",
        "\n",
        "!python inference.py --checkpoint_path 'checkpoints/wav2lip_gan.pth' --face $video_path_fix --audio \"/content/output_synth.wav\" --pads $pad_top $pad_bottom $pad_left $pad_right --resize_factor $rescaleFactor --nosmooth --outfile '/content/output_video.mp4'\n"
      ],
      "metadata": {
        "id": "WfrknDhGPrtz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End"
      ],
      "metadata": {
        "id": "jCP5PRe6aXlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to get the video/s and download links\n",
        "from google.colab import files\n",
        "from IPython.core.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "import base64\n",
        "import os\n",
        "\n",
        "# List of video paths to check\n",
        "video_paths = [\"/content/output_video.mp4\", \"/content/output_high_qual.mp4\"]\n",
        "\n",
        "# Download function\n",
        "def download_video(b):\n",
        "    files.download(b.video_path)\n",
        "\n",
        "# Create button widget for download\n",
        "download_buttons = []\n",
        "\n",
        "# Layout definition for button\n",
        "button_layout = widgets.Layout(width='250px')\n",
        "\n",
        "# Loop through each video path to check for existence and display\n",
        "for video_path in video_paths:\n",
        "    if os.path.exists(video_path):\n",
        "        # Encode video to base64\n",
        "        with open(video_path, \"rb\") as video_file:\n",
        "            video_base64 = base64.b64encode(video_file.read()).decode()\n",
        "\n",
        "        # Create HTML widget for video\n",
        "        video_html = HTML(data=f\"\"\"\n",
        "        <video width=400 controls>\n",
        "            <source src=\"data:video/mp4;base64,{video_base64}\" type=\"video/mp4\" />\n",
        "        </video>\n",
        "        \"\"\")\n",
        "\n",
        "        # Create button widget for download and link to the video path\n",
        "        download_button = widgets.Button(description=f\"Download {os.path.basename(video_path)}\",\n",
        "                                         layout=button_layout)\n",
        "        download_button.video_path = video_path\n",
        "        download_button.on_click(download_video)\n",
        "        download_buttons.append(download_button)\n",
        "\n",
        "        # Display widgets\n",
        "        display(video_html)\n",
        "        display(download_button)\n"
      ],
      "metadata": {
        "id": "C1hHhIy6W3Zi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}